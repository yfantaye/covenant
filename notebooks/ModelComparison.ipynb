{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Performance Visualization\n",
    "\n",
    "This notebook compares different model outputs and visualizes their performance using various metrics including:\n",
    "- Score distributions for failed vs non-failed companies\n",
    "- Smoothed P(failure|Score) curves\n",
    "- Model weights comparison\n",
    "- Performance metrics\n",
    "\n",
    "## Models to Compare:\n",
    "- Baseline model (to be calculated)\n",
    "- Scottv1 model\n",
    "- Scottv2 model\n",
    "- Any additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Reload all modules automatically\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#%autoreload\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#%autoreload 2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnotebook_setup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load baseline model\u001b[39;00m\n\u001b[32m      8\u001b[39m baseline_model = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mcommon_data/scott_baseline.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Reload all modules automatically\n",
    "#%autoreload\n",
    "#%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations set up!\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "base_path = Path('../')\n",
    "model_outputs_path = base_path / 'model_outputs'\n",
    "data_path = base_path / '../covenant/SampleCovenantData'\n",
    "\n",
    "# Model configurations\n",
    "models = {\n",
    "    'baseline': {\n",
    "        'path': model_outputs_path / 'baseline',\n",
    "        'color': 'gray',\n",
    "        'linestyle': '--'\n",
    "    },\n",
    "    'scottv1': {\n",
    "        'path': model_outputs_path / 'scottv1',\n",
    "        'color': 'blue',\n",
    "        'linestyle': '-'\n",
    "    },\n",
    "    'scottv2': {\n",
    "        'path': model_outputs_path / 'scottv2',\n",
    "        'color': 'red',\n",
    "        'linestyle': '-'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model configurations set up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "baseline_model = pd.read_csv('../common_data/scott_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(model_name, model_config):\n",
    "    \"\"\"Load scores and weights for a given model\"\"\"\n",
    "    try:\n",
    "        scores_path = model_config['path'] / 'scores.csv'\n",
    "        weights_path = model_config['path'] / 'weights.csv'\n",
    "        \n",
    "        if scores_path.exists() and weights_path.exists():\n",
    "            scores = pd.read_csv(scores_path)\n",
    "            weights = pd.read_csv(weights_path)\n",
    "            \n",
    "            # Convert signal_date to datetime\n",
    "            scores['signal_date'] = pd.to_datetime(scores['signal_date'])\n",
    "            \n",
    "            return scores, weights\n",
    "        else:\n",
    "            print(f\"Missing files for {model_name}\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load all available model data\n",
    "model_data = {}\n",
    "for model_name, model_config in models.items():\n",
    "    scores, weights = load_model_data(model_name, model_config)\n",
    "    if scores is not None:\n",
    "        model_data[model_name] = {\n",
    "            'scores': scores,\n",
    "            'weights': weights,\n",
    "            'config': model_config\n",
    "        }\n",
    "        print(f\"Loaded {model_name}: {len(scores)} records\")\n",
    "\n",
    "print(f\"\\nLoaded {len(model_data)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distributions(model_data):\n",
    "    \"\"\"Plot score distributions for failed vs non-failed companies\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(model_data), figsize=(6*len(model_data), 5))\n",
    "    if len(model_data) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(model_data.items()):\n",
    "        scores_df = data['scores']\n",
    "        \n",
    "        # Plot distributions\n",
    "        sns.kdeplot(data=scores_df[scores_df['fail'] == 1], x='score', \n",
    "                   label='Failed', fill=True, common_norm=False, ax=axes[i])\n",
    "        sns.kdeplot(data=scores_df[scores_df['fail'] == 0], x='score', \n",
    "                   label='Non-failed', fill=True, common_norm=False, ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(f'{model_name.upper()} Score Distribution')\n",
    "        axes[i].set_xlabel('Score')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend()\n",
    "        \n",
    "        # Add statistics\n",
    "        failed_mean = scores_df[scores_df['fail'] == 1]['score'].mean()\n",
    "        non_failed_mean = scores_df[scores_df['fail'] == 0]['score'].mean()\n",
    "        axes[i].text(0.02, 0.98, f'Failed mean: {failed_mean:.3f}\\nNon-failed mean: {non_failed_mean:.3f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_score_distributions(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smoothed P(failure|Score) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smoothed_failure_probability(scores_df, window_size=100):\n",
    "    \"\"\"Calculate smoothed P(failure|Score) using rolling average\"\"\"\n",
    "    # Sort by score\n",
    "    df_sorted = scores_df.sort_values('score').reset_index(drop=True)\n",
    "    \n",
    "    # Calculate rolling average of failure probability\n",
    "    rolling_avg = df_sorted['fail'].rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    return df_sorted['score'], rolling_avg\n",
    "\n",
    "def plot_smoothed_failure_probability(model_data, window_size=100):\n",
    "    \"\"\"Plot smoothed P(failure|Score) for all models\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        scores_df = data['scores']\n",
    "        config = data['config']\n",
    "        \n",
    "        scores, probs = calculate_smoothed_failure_probability(scores_df, window_size)\n",
    "        \n",
    "        plt.plot(scores, probs, \n",
    "                color=config['color'], \n",
    "                linestyle=config['linestyle'],\n",
    "                linewidth=2, \n",
    "                label=f'{model_name.upper()}')\n",
    "    \n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('P(failure)')\n",
    "    plt.title(f'Smoothed P(failure | Score) - Window Size: {window_size}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with different window sizes\n",
    "for window_size in [50, 100, 200]:\n",
    "    plot_smoothed_failure_probability(model_data, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Weights Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_comparison(model_data, top_n=15):\n",
    "    \"\"\"Compare weights across models\"\"\"\n",
    "    # Create a combined dataframe for weights\n",
    "    weights_df = pd.DataFrame()\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        weights = data['weights'].set_index('Feature')['Weight']\n",
    "        weights_df[model_name] = weights\n",
    "    \n",
    "    # Get top features by absolute weight across all models\n",
    "    max_abs_weights = weights_df.abs().max(axis=1)\n",
    "    top_features = max_abs_weights.nlargest(top_n).index\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Heatmap\n",
    "    weights_subset = weights_df.loc[top_features]\n",
    "    sns.heatmap(weights_subset, annot=True, cmap='RdBu_r', center=0, ax=ax1)\n",
    "    ax1.set_title('Feature Weights Comparison (Heatmap)')\n",
    "    \n",
    "    # Bar plot\n",
    "    weights_subset.plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Feature Weights Comparison (Bar Plot)')\n",
    "    ax2.set_xlabel('Features')\n",
    "    ax2.set_ylabel('Weight')\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_weights_comparison(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(scores_df):\n",
    "    \"\"\"Calculate various performance metrics\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(scores_df['fail'], scores_df['score'])\n",
    "    \n",
    "    # PR AUC\n",
    "    precision, recall, _ = precision_recall_curve(scores_df['fail'], scores_df['score'])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Score statistics\n",
    "    failed_scores = scores_df[scores_df['fail'] == 1]['score']\n",
    "    non_failed_scores = scores_df[scores_df['fail'] == 0]['score']\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'failed_mean': failed_scores.mean(),\n",
    "        'non_failed_mean': non_failed_scores.mean(),\n",
    "        'failed_std': failed_scores.std(),\n",
    "        'non_failed_std': non_failed_scores.std(),\n",
    "        'separation': (failed_scores.mean() - non_failed_scores.mean()) / np.sqrt((failed_scores.var() + non_failed_scores.var()) / 2)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate and display metrics for all models\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for model_name, data in model_data.items():\n",
    "    metrics = calculate_performance_metrics(data['scores'])\n",
    "    metrics_df[model_name] = metrics\n",
    "\n",
    "metrics_df = metrics_df.T\n",
    "print(\"Performance Metrics:\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for model_name, data in model_data.items():\n",
    "    scores_df = data['scores']\n",
    "    config = data['config']\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(scores_df['fail'], scores_df['score'])\n",
    "    plt.plot(fpr, tpr, color=config['color'], linestyle=config['linestyle'], \n",
    "             linewidth=2, label=f'{model_name.upper()} (AUC: {metrics_df.loc[model_name, \"roc_auc\"]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for model_name, data in model_data.items():\n",
    "    scores_df = data['scores']\n",
    "    config = data['config']\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(scores_df['fail'], scores_df['score'])\n",
    "    plt.plot(recall, precision, color=config['color'], linestyle=config['linestyle'], \n",
    "             linewidth=2, label=f'{model_name.upper()} (AUC: {metrics_df.loc[model_name, \"pr_auc\"]:.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Model Calculation\n",
    "\n",
    "This section calculates a baseline model for comparison. The baseline uses equal weights for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline_model():\n",
    "    \"\"\"Calculate baseline model with equal weights\"\"\"\n",
    "    # Load signals data\n",
    "    signals_file = data_path / 'binarySignalsPart00.csv'\n",
    "    if not signals_file.exists():\n",
    "        print(\"Signals file not found. Skipping baseline calculation.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load signals\n",
    "    signals_df = pd.read_csv(signals_file, parse_dates=['signal_date'], date_format='%m/%d/%Y')\n",
    "    \n",
    "    # Get feature columns (exclude companyid and signal_date)\n",
    "    feature_cols = [col for col in signals_df.columns if col not in ['companyid', 'signal_date']]\n",
    "    \n",
    "    # Create equal weights\n",
    "    equal_weights = pd.Series(1.0/len(feature_cols), index=feature_cols)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = signals_df[['companyid', 'signal_date']].copy()\n",
    "    scores['score'] = signals_df[feature_cols].dot(equal_weights)\n",
    "    scores['fail'] = 0  # Placeholder - would need actual failure data\n",
    "    \n",
    "    # Save baseline results\n",
    "    baseline_path = model_outputs_path / 'baseline'\n",
    "    baseline_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save weights\n",
    "    weights_df = equal_weights.reset_index()\n",
    "    weights_df.columns = ['Feature', 'Weight']\n",
    "    weights_df.to_csv(baseline_path / 'weights.csv', index=False)\n",
    "    \n",
    "    # Save scores\n",
    "    scores.to_csv(baseline_path / 'scores.csv', index=False)\n",
    "    \n",
    "    print(f\"Baseline model saved to {baseline_path}\")\n",
    "    return scores, equal_weights\n",
    "\n",
    "# Uncomment to calculate baseline\n",
    "# baseline_scores, baseline_weights = calculate_baseline_model()\n",
    "# if baseline_scores is not None:\n",
    "#     # Reload model data to include baseline\n",
    "#     scores, weights = load_model_data('baseline', models['baseline'])\n",
    "#     if scores is not None:\n",
    "#         model_data['baseline'] = {\n",
    "#             'scores': scores,\n",
    "#             'weights': weights,\n",
    "#             'config': models['baseline']\n",
    "#         }\n",
    "#         print(\"Baseline model loaded successfully!\")\n",
    "\n",
    "print(\"Baseline calculation function ready. Uncomment to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== MODEL COMPARISON SUMMARY ===\\n\")\n",
    "\n",
    "for model_name, data in model_data.items():\n",
    "    scores_df = data['scores']\n",
    "    \n",
    "    print(f\"{model_name.upper()}:\")\n",
    "    print(f\"  Total records: {len(scores_df):,}\")\n",
    "    print(f\"  Failed companies: {scores_df['fail'].sum():,}\")\n",
    "    print(f\"  Non-failed companies: {(scores_df['fail'] == 0).sum():,}\")\n",
    "    print(f\"  Score range: {scores_df['score'].min():.3f} to {scores_df['score'].max():.3f}\")\n",
    "    print(f\"  Score std: {scores_df['score'].std():.3f}\")\n",
    "    \n",
    "    if model_name in metrics_df.index:\n",
    "        print(f\"  ROC AUC: {metrics_df.loc[model_name, 'roc_auc']:.3f}\")\n",
    "        print(f\"  PR AUC: {metrics_df.loc[model_name, 'pr_auc']:.3f}\")\n",
    "        print(f\"  Separation: {metrics_df.loc[model_name, 'separation']:.3f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=== KEY INSIGHTS ===\")\n",
    "print(\"1. The smoothed P(failure|Score) curves show how well each model separates failed from non-failed companies\")\n",
    "print(\"2. Higher separation between failed and non-failed score distributions indicates better performance\")\n",
    "print(\"3. ROC AUC and PR AUC provide quantitative measures of model performance\")\n",
    "print(\"4. Feature weights show which signals are most important for each model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
