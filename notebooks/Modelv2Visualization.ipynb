{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelv2 Output Visualization\n",
    "\n",
    "This notebook analyzes and visualizes the outputs from the modelv2 pipeline, including:\n",
    "- Score distributions and analysis\n",
    "- Feature weights analysis\n",
    "- Signal correlation analysis\n",
    "- Performance metrics\n",
    "- Time series analysis of scores\n",
    "\n",
    "## Data Sources:\n",
    "- Model outputs: `model_outputs/{model_name}/`\n",
    "- Signal data: `../covenant/SampleCovenantData/`\n",
    "- Configuration: `config_local.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../config_local.yaml')\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}: {dict(value)}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = Path('../')\n",
    "model_outputs_path = base_path / 'model_outputs'\n",
    "data_path = Path(config['data_directory'])\n",
    "\n",
    "# Available models\n",
    "available_models = [d.name for d in model_outputs_path.iterdir() if d.is_dir()]\n",
    "print(f\"Available models: {available_models}\")\n",
    "\n",
    "# Load model data\n",
    "model_data = {}\n",
    "for model_name in available_models:\n",
    "    model_path = model_outputs_path / model_name\n",
    "    scores_file = model_path / 'scores.csv'\n",
    "    weights_file = model_path / 'weights.csv'\n",
    "    \n",
    "    if scores_file.exists() and weights_file.exists():\n",
    "        scores = pd.read_csv(scores_file)\n",
    "        weights = pd.read_csv(weights_file)\n",
    "        \n",
    "        # Convert dates\n",
    "        scores['signal_date'] = pd.to_datetime(scores['signal_date'])\n",
    "        \n",
    "        model_data[model_name] = {\n",
    "            'scores': scores,\n",
    "            'weights': weights\n",
    "        }\n",
    "        print(f\"Loaded {model_name}: {len(scores):,} records\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(model_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Score Analysis and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scores(model_name, scores_df):\n",
    "    \"\"\"Comprehensive score analysis\"\"\"\n",
    "    print(f\"\\n=== {model_name.upper()} SCORE ANALYSIS ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Total records: {len(scores_df):,}\")\n",
    "    print(f\"Failed companies: {scores_df['fail'].sum():,}\")\n",
    "    print(f\"Non-failed companies: {(scores_df['fail'] == 0).sum():,}\")\n",
    "    print(f\"Failure rate: {scores_df['fail'].mean():.3%}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    failed_scores = scores_df[scores_df['fail'] == 1]['score']\n",
    "    non_failed_scores = scores_df[scores_df['fail'] == 0]['score']\n",
    "    \n",
    "    print(f\"\\nScore Statistics:\")\n",
    "    print(f\"  Overall - Mean: {scores_df['score'].mean():.4f}, Std: {scores_df['score'].std():.4f}\")\n",
    "    print(f\"  Failed - Mean: {failed_scores.mean():.4f}, Std: {failed_scores.std():.4f}\")\n",
    "    print(f\"  Non-failed - Mean: {non_failed_scores.mean():.4f}, Std: {non_failed_scores.std():.4f}\")\n",
    "    \n",
    "    # Separation metric\n",
    "    separation = (failed_scores.mean() - non_failed_scores.mean()) / np.sqrt((failed_scores.var() + non_failed_scores.var()) / 2)\n",
    "    print(f\"  Separation: {separation:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'failed_scores': failed_scores,\n",
    "        'non_failed_scores': non_failed_scores,\n",
    "        'separation': separation\n",
    "    }\n",
    "\n",
    "# Analyze all models\n",
    "score_analysis = {}\n",
    "for model_name, data in model_data.items():\n",
    "    score_analysis[model_name] = analyze_scores(model_name, data['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "fig, axes = plt.subplots(1, len(model_data), figsize=(6*len(model_data), 5))\n",
    "if len(model_data) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (model_name, data) in enumerate(model_data.items()):\n",
    "    scores_df = data['scores']\n",
    "    \n",
    "    # Plot distributions\n",
    "    sns.kdeplot(data=scores_df[scores_df['fail'] == 1], x='score', \n",
    "               label='Failed', fill=True, common_norm=False, ax=axes[i])\n",
    "    sns.kdeplot(data=scores_df[scores_df['fail'] == 0], x='score', \n",
    "               label='Non-failed', fill=True, common_norm=False, ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f'{model_name.upper()} Score Distribution')\n",
    "    axes[i].set_xlabel('Score')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    \n",
    "    # Add statistics\n",
    "    failed_mean = scores_df[scores_df['fail'] == 1]['score'].mean()\n",
    "    non_failed_mean = scores_df[scores_df['fail'] == 0]['score'].mean()\n",
    "    axes[i].text(0.02, 0.98, f'Failed mean: {failed_mean:.3f}\\nNon-failed mean: {non_failed_mean:.3f}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smoothed P(failure|Score) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smoothed_failure_probability(scores_df, window_size=100):\n",
    "    \"\"\"Calculate smoothed P(failure|Score) using rolling average\"\"\"\n",
    "    # Sort by score\n",
    "    df_sorted = scores_df.sort_values('score').reset_index(drop=True)\n",
    "    \n",
    "    # Calculate rolling average of failure probability\n",
    "    rolling_avg = df_sorted['fail'].rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    return df_sorted['score'], rolling_avg\n",
    "\n",
    "# Plot smoothed failure probability for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "for i, (model_name, data) in enumerate(model_data.items()):\n",
    "    scores_df = data['scores']\n",
    "    \n",
    "    scores, probs = calculate_smoothed_failure_probability(scores_df, window_size=100)\n",
    "    \n",
    "    plt.plot(scores, probs, \n",
    "            color=colors[i % len(colors)], \n",
    "            linewidth=2, \n",
    "            label=f'{model_name.upper()}')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('P(failure)')\n",
    "plt.title('Smoothed P(failure | Score) - Window Size: 100')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot with different window sizes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "window_sizes = [50, 100, 200]\n",
    "\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    for j, (model_name, data) in enumerate(model_data.items()):\n",
    "        scores_df = data['scores']\n",
    "        scores, probs = calculate_smoothed_failure_probability(scores_df, window_size)\n",
    "        \n",
    "        axes[i].plot(scores, probs, \n",
    "                    color=colors[j % len(colors)], \n",
    "                    linewidth=2, \n",
    "                    label=f'{model_name.upper()}')\n",
    "    \n",
    "    axes[i].set_xlabel('Score')\n",
    "    axes[i].set_ylabel('P(failure)')\n",
    "    axes[i].set_title(f'Window Size: {window_size}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Weights Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weights(model_name, weights_df):\n",
    "    \"\"\"Analyze feature weights\"\"\"\n",
    "    print(f\"\\n=== {model_name.upper()} WEIGHTS ANALYSIS ===\")\n",
    "    \n",
    "    weights = weights_df.set_index('Feature')['Weight']\n",
    "    \n",
    "    print(f\"Total features: {len(weights)}\")\n",
    "    print(f\"Positive weights: {(weights > 0).sum()}\")\n",
    "    print(f\"Negative weights: {(weights < 0).sum()}\")\n",
    "    print(f\"Zero weights: {(weights == 0).sum()}\")\n",
    "    \n",
    "    print(f\"\\nWeight Statistics:\")\n",
    "    print(f\"  Mean: {weights.mean():.4f}\")\n",
    "    print(f\"  Std: {weights.std():.4f}\")\n",
    "    print(f\"  Min: {weights.min():.4f}\")\n",
    "    print(f\"  Max: {weights.max():.4f}\")\n",
    "    \n",
    "    # Top positive and negative weights\n",
    "    print(f\"\\nTop 5 Positive Weights:\")\n",
    "    for feature, weight in weights.nlargest(5).items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 Negative Weights:\")\n",
    "    for feature, weight in weights.nsmallest(5).items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Analyze weights for all models\n",
    "weights_analysis = {}\n",
    "for model_name, data in model_data.items():\n",
    "    weights_analysis[model_name] = analyze_weights(model_name, data['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weights comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Create weights dataframe for comparison\n",
    "weights_df = pd.DataFrame()\n",
    "for model_name, weights in weights_analysis.items():\n",
    "    weights_df[model_name] = weights\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(weights_df, annot=True, cmap='RdBu_r', center=0, ax=axes[0])\n",
    "axes[0].set_title('Feature Weights Comparison (Heatmap)')\n",
    "\n",
    "# Bar plot for top features\n",
    "top_features = weights_df.abs().max(axis=1).nlargest(15).index\n",
    "weights_subset = weights_df.loc[top_features]\n",
    "weights_subset.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Top 15 Feature Weights Comparison')\n",
    "axes[1].set_xlabel('Features')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_signals():\n",
    "    \"\"\"Load and analyze signal data\"\"\"\n",
    "    signals_file = data_path / 'binarySignalsPart00.csv'\n",
    "    \n",
    "    if not signals_file.exists():\n",
    "        print(f\"Signals file not found: {signals_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Load signals\n",
    "    signals_df = pd.read_csv(signals_file, parse_dates=['signal_date'], date_format='%m/%d/%Y')\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in signals_df.columns if col not in ['companyid', 'signal_date']]\n",
    "    \n",
    "    print(f\"Signal Analysis:\")\n",
    "    print(f\"  Total records: {len(signals_df):,}\")\n",
    "    print(f\"  Unique companies: {signals_df['companyid'].nunique():,}\")\n",
    "    print(f\"  Date range: {signals_df['signal_date'].min()} to {signals_df['signal_date'].max()}\")\n",
    "    print(f\"  Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Signal frequency analysis\n",
    "    signal_freq = signals_df[feature_cols].mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nSignal Frequencies (Top 10):\")\n",
    "    for feature, freq in signal_freq.head(10).items():\n",
    "        print(f\"  {feature}: {freq:.3%}\")\n",
    "    \n",
    "    return signals_df, feature_cols, signal_freq\n",
    "\n",
    "# Load signals\n",
    "signals_data = load_and_analyze_signals()\n",
    "if signals_data is not None:\n",
    "    signals_df, feature_cols, signal_freq = signals_data\n",
    "else:\n",
    "    signals_df, feature_cols, signal_freq = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if signals_df is not None:\n",
    "    # Plot signal frequencies\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    signal_freq.plot(kind='bar')\n",
    "    plt.title('Signal Frequencies')\n",
    "    plt.xlabel('Signals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    signal_freq.hist(bins=20, alpha=0.7)\n",
    "    plt.title('Distribution of Signal Frequencies')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    if len(feature_cols) > 1:\n",
    "        corr_matrix = signals_df[feature_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "        plt.title('Signal Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highly correlated signals\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.5:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(\"\\nHighly Correlated Signal Pairs (|correlation| > 0.5):\")\n",
    "            for pair in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(scores_df):\n",
    "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(scores_df['fail'], scores_df['score'])\n",
    "    \n",
    "    # PR AUC\n",
    "    precision, recall, _ = precision_recall_curve(scores_df['fail'], scores_df['score'])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Score statistics\n",
    "    failed_scores = scores_df[scores_df['fail'] == 1]['score']\n",
    "    non_failed_scores = scores_df[scores_df['fail'] == 0]['score']\n",
    "    \n",
    "    # Separation metric\n",
    "    separation = (failed_scores.mean() - non_failed_scores.mean()) / np.sqrt((failed_scores.var() + non_failed_scores.var()) / 2)\n",
    "    \n",
    "    # Additional metrics\n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'separation': separation,\n",
    "        'failed_mean': failed_scores.mean(),\n",
    "        'non_failed_mean': non_failed_scores.mean(),\n",
    "        'failed_std': failed_scores.std(),\n",
    "        'non_failed_std': non_failed_scores.std(),\n",
    "        'score_range': scores_df['score'].max() - scores_df['score'].min(),\n",
    "        'score_std': scores_df['score'].std()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for all models\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for model_name, data in model_data.items():\n",
    "    metrics = calculate_performance_metrics(data['scores'])\n",
    "    metrics_df[model_name] = metrics\n",
    "\n",
    "metrics_df = metrics_df.T\n",
    "print(\"Performance Metrics:\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and PR curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC curves\n",
    "for model_name, data in model_data.items():\n",
    "    scores_df = data['scores']\n",
    "    fpr, tpr, _ = roc_curve(scores_df['fail'], scores_df['score'])\n",
    "    roc_auc = roc_auc_score(scores_df['fail'], scores_df['score'])\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, linewidth=2, \n",
    "                 label=f'{model_name.upper()} (AUC: {roc_auc:.3f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR curves\n",
    "for model_name, data in model_data.items():\n",
    "    scores_df = data['scores']\n",
    "    precision, recall, _ = precision_recall_curve(scores_df['fail'], scores_df['score'])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    axes[1].plot(recall, precision, linewidth=2, \n",
    "                 label=f'{model_name.upper()} (AUC: {pr_auc:.3f})')\n",
    "\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_series(scores_df):\n",
    "    \"\"\"Analyze scores over time\"\"\"\n",
    "    # Add time components\n",
    "    scores_df = scores_df.copy()\n",
    "    scores_df['year'] = scores_df['signal_date'].dt.year\n",
    "    scores_df['month'] = scores_df['signal_date'].dt.month\n",
    "    scores_df['quarter'] = scores_df['signal_date'].dt.quarter\n",
    "    \n",
    "    # Time series analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Score trends over time\n",
    "    yearly_stats = scores_df.groupby('year')['score'].agg(['mean', 'std']).reset_index()\n",
    "    axes[0, 0].plot(yearly_stats['year'], yearly_stats['mean'], marker='o')\n",
    "    axes[0, 0].fill_between(yearly_stats['year'], \n",
    "                           yearly_stats['mean'] - yearly_stats['std'],\n",
    "                           yearly_stats['mean'] + yearly_stats['std'], alpha=0.3)\n",
    "    axes[0, 0].set_title('Average Score by Year')\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Failure rate over time\n",
    "    failure_rate = scores_df.groupby('year')['fail'].mean()\n",
    "    axes[0, 1].plot(failure_rate.index, failure_rate.values, marker='o', color='red')\n",
    "    axes[0, 1].set_title('Failure Rate by Year')\n",
    "    axes[0, 1].set_xlabel('Year')\n",
    "    axes[0, 1].set_ylabel('Failure Rate')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Score distribution by quarter\n",
    "    quarterly_stats = scores_df.groupby('quarter')['score'].mean()\n",
    "    axes[1, 0].bar(quarterly_stats.index, quarterly_stats.values)\n",
    "    axes[1, 0].set_title('Average Score by Quarter')\n",
    "    axes[1, 0].set_xlabel('Quarter')\n",
    "    axes[1, 0].set_ylabel('Average Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Score vs failure rate scatter\n",
    "    yearly_score_fail = scores_df.groupby('year').agg({\n",
    "        'score': 'mean',\n",
    "        'fail': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    axes[1, 1].scatter(yearly_score_fail['score'], yearly_score_fail['fail'])\n",
    "    for _, row in yearly_score_fail.iterrows():\n",
    "        axes[1, 1].annotate(str(int(row['year'])), \n",
    "                           (row['score'], row['fail']),\n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 1].set_xlabel('Average Score')\n",
    "    axes[1, 1].set_ylabel('Failure Rate')\n",
    "    axes[1, 1].set_title('Score vs Failure Rate by Year')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return yearly_stats, failure_rate\n",
    "\n",
    "# Analyze time series for each model\n",
    "for model_name, data in model_data.items():\n",
    "    print(f\"\\n=== {model_name.upper()} TIME SERIES ANALYSIS ===\")\n",
    "    yearly_stats, failure_rate = analyze_time_series(data['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODELV2 VISUALIZATION SUMMARY ===\\n\")\n",
    "\n",
    "# Model comparison summary\n",
    "print(\"Model Performance Summary:\")\n",
    "for model_name in model_data.keys():\n",
    "    if model_name in metrics_df.index:\n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        print(f\"  ROC AUC: {metrics_df.loc[model_name, 'roc_auc']:.3f}\")\n",
    "        print(f\"  PR AUC: {metrics_df.loc[model_name, 'pr_auc']:.3f}\")\n",
    "        print(f\"  Separation: {metrics_df.loc[model_name, 'separation']:.3f}\")\n",
    "        print(f\"  Score Range: {metrics_df.loc[model_name, 'score_range']:.3f}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"1. The smoothed P(failure|Score) curves show the relationship between model scores and failure probability\")\n",
    "print(\"2. Higher separation between failed and non-failed distributions indicates better model performance\")\n",
    "print(\"3. ROC AUC and PR AUC provide quantitative measures of model discrimination ability\")\n",
    "print(\"4. Feature weights reveal which signals are most important for each model variant\")\n",
    "print(\"5. Time series analysis shows how model performance varies over time\")\n",
    "print(\"6. Signal correlation analysis helps understand feature relationships and potential redundancy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 